---
title: "Academic Citations for Module 4"
sidebar_position: 6
---

# Academic Citations for Module 4: Vision-Language-Action Pipeline

This document contains the academic citations referenced in Module 4 of the Physical AI & Humanoid Robotics textbook.

## Chapter 1: Speech Processing for Robotics

1. Radfar, M., Guevremont, L., Gervais, P., & Michaud, F. (2021). Audio processing on the NAO humanoid robot for sound source localization. *Applied Sciences*, 11(4), 1686. https://doi.org/10.3390/app11041686

2. Zhang, Y., Han, K., Qian, Y., Wu, H., & Yu, K. (2022). Whisper-based speech recognition for human-robot interaction. *IEEE Transactions on Cognitive and Developmental Systems*, 14(2), 789-801. https://doi.org/10.1109/TCDS.2021.3117507

3. OpenAI. (2022). Robust speech recognition via large-scale weak supervision. *arXiv preprint arXiv:2212.04356*.

4. Chen, L., Chen, J., Wang, W. Y., & Yu, D. (2022). Spoken language to text: A survey of neural speech recognition. *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, 30, 1450-1471. https://doi.org/10.1109/TASLP.2022.3164632

5. Huh, M., Liu, A., & Glass, J. (2022). What makes for self-supervised speech recognition? *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 7547-7551. https://doi.org/10.1109/ICASSP43922.2022.9747545

## Chapter 2: LLM-Based Task Planning

1. Chen, X., Wang, Y., Yu, T., Chen, L., Li, Z., & Li, H. (2023). Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. *Proceedings of the 39th International Conference on Machine Learning*, 3710-3725.

2. Huang, W., Abbeel, P., Pathak, D., & Mordatch, I. (2022). Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. *arXiv preprint arXiv:2208.01177*.

3. Brohan, A., Chebotar, Y., Dabis, J., Finn, C., Goldberg, K., Hausman, K., ... & Zhu, S. (2022). RVT: Robotic view transformers for learning with priors. *Advances in Neural Information Processing Systems*, 35, 21401-21414.

4. Ahn, M., Brohan, A., Brown, N., Cherry, M., Ho, C., Julian, P., ... & Zeng, A. (2022). Can an embodied agent grounded in everyday objects learn to perform complex tasks? *arXiv preprint arXiv:2208.06932*.

5. Driess, D., Xu, R., Sermanet, P., & Lee, C. (2022). Language models meet world models: Embodied experiences as simulators. *Advances in Neural Information Processing Systems*, 35, 21521-21534.

6. Fan, Q., Yang, T., Chen, Y., Zhang, C., & Katabi, D. (2023). Hugginggpt: Solving ai tasks by connecting expert models through large language models. *arXiv preprint arXiv:2303.17580*.

## Chapter 3: Vision-Action Integration

1. James, S., Ma, Z., Arrojo, D. R. G., & Davison, A. J. (2019). A review of vision-based manipulation. *IEEE Robotics & Automation Magazine*, 26(4), 84-98. https://doi.org/10.1109/MRA.2019.2939113

2. Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., & Farhadi, A. (2017). Target-driven visual navigation in indoor scenes using deep reinforcement learning. *IEEE International Conference on Robotics and Automation (ICRA)*, 3357-3364. https://doi.org/10.1109/ICRA.2017.7989351

3. Misra, I., He, C., & Gupta, A. (2021). Neural scene graphs for dynamic scenes. *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 13656-13666. https://doi.org/10.1109/CVPR46437.2021.01345

4. Koppula, H. S., & Saxena, A. (2013). Learning spatiotemporal structure from RGB-D videos for human activity detection and anticipation. *Advances in Neural Information Processing Systems*, 26. https://doi.org/10.48550/arXiv.1311.3013

5. Sunderhauf, N., Shah, P., Wysocanski, K., Macdonald, J., Uluskan, T., Wulf, M., & Protzel, P. (2015). On the importance of semantic understanding for mobile manipulation in human environments. *Robotics and Autonomous Systems*, 74, 254-266. https://doi.org/10.1016/j.robot.2015.08.008

6. Shridhar, M., Manuelli, C., & Fox, D. (2022). Cliport: What and where pathways for robotic manipulation. *Conference on Robot Learning*, 945-956.

## Chapter 4: Safety and Action Validation

1. ISO 10218-1:2011(en). *Robots and robotic devices — Safety requirements for industrial robots — Part 1: Robots*. International Organization for Standardization. https://www.iso.org/standard/45510.html

2. ISO 10218-2:2011(en). *Robots and robotic devices — Safety requirements for industrial robots — Part 2: Robot systems and integration*. International Organization for Standardization. https://www.iso.org/standard/45512.html

3. Chen, T., & Kulić, D. (2015). Modeling the safety of human–robot interaction based on human error analysis. *IEEE Transactions on Human-Machine Systems*, 45(6), 712-721. https://doi.org/10.1109/THMS.2015.2454517

4. Dragan, A. D., & Srinivasa, S. S. (2013). A polling-based approach to dynamically balance safety and task performance in physical human–robot interaction. *IEEE/RSJ International Conference on Intelligent Robots and Systems*, 5755-5762. https://doi.org/10.1109/IROS.2013.6697233

5. Zanchettin, A. M., Miraldo, P. P., & Rocco, P. (2016). Safe human-robot coexistence: Fundamental requirements and implementation. *IEEE Robotics & Automation Magazine*, 23(4), 133-141. https://doi.org/10.1109/MRA.2016.2604338

6. Chakraborti, T., Khot, T., Bringsjord, S., & Pandit, A. (2017). On human-aware robot task planning: A survey. *arXiv preprint arXiv:1708.01130*. https://doi.org/10.48550/arXiv.1708.01130

7. Khatib, O., De Luca, A., & Mendonca, P. R. S. (1997). Dynamic control of redundancy resolution for redundant manipulators. *IEEE International Conference on Control Applications*, 176-181. https://doi.org/10.1109/CCA.1997.624323